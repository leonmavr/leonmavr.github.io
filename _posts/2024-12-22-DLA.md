---
layout: post
title: "Understanding Simulating Diffusion-Limited Aggregation and Simulating It In C"
mathjax: true
categories: c
tag: c
---

# 0. Motivation and Goals

A few months ago, I saw Diffusion-limited aggregation (DLA) in some abstract art, got curious about the rules that 
dictate its pattern, then wanted to know how it can be implemented, and fell into a rabbit hole.

DLA is one of the most intriguing and ubiquitous natural phenomena. It's interesting not
only because of the patterns it gives rise to, but because it occurs both in microscopic and macroscopic phenomena, such as:

* Dielectric breakdown: fractal patterns in electrical discharge.
* Bacterial colony growth.
* Mineral dendrites: branch-like patterns in crystals.
* Crystalization of snowflakes.
* Coral reef growth.

It gives rise to structure out of randomness and to some very interesting fractals as you can see below.

**TODO** DLA pics in nature

DLA occurs when an environment has the following properties:

1. Random motion of particles (hence the "diffusion" part of DLA)
2. Sticky aggregation (hence the "aggregation" part) - i.e. when a particle collides with the core, it
irreversibly sticks to it.

To understand it, it's important to get a solid understanding of random walks and combine it with the aggregation principle.
I will also simulate it in C, since I wanted to write my own quick 3D sphere renderer on top of the `SDL` library anyway.
The goals of this article are to:

1. Show you the basic math prerequisites to understand DLA.
2. Understand random walks both microscopically (by deriving the differential equation governing the particle motion)
3. Understand random walks macroscopically (by deriving the average spread of particles)
4. Understand the basics of aggregation and how it gives rise to fractals (I will not derive the maths of fractals)
5. Simulate DLA in real time in 3D with interactive camera controls.

Unraveling the theory is just as interesting as the simulation so grab a drink and prepare for a long read.

# 1. Random Walk Experiments

The principle of a random walker's motion is simple. The motion is confided in a lattice (e.g. a uniform grid 
in 2D). At each step, the random walker must step in one of the neighboring cells of the lattice. Consider the 
simple case of 1D discrete motion, then at the next step the walker will move either left or right ($\pm1$).
The random walk in this case in unbiased, so the $+1$ and $-1$ displacements are equiprobable.
It's often interesting to look at the motion of $N$ random walkers as a whole as opposed to just one in other to
measure the macroscoping properties of the system.

The quantities we're interested in measuring are:

1. Position
2. Mean position of all walkers
3. Mean squared distance (MSD) over all walkers. 
4. Probability density function (PDF), i.e. the probability that a walker will be at some position.

To be 100% sure you know how to calculate MSD, consider the following table of positions $x_i(t)$ below for 3 walkers. MSD
 is the expectation of the square of the displacement - it is not the same as the mean squared!

| time ($t$) | $x_1(t)$ | $x_2(t)$ | $x_3(t)$ | $MSD = \frac{1}{N}\sum\limits_{i=1}^{N=3}x_i^2$ |
|------------|------------|------------|------------|-----------------------------------------------------|
| $0$        | $0 $       | $ 0$       | $ 0$       | $0$                                                 |
| $1$        | $+1$       | $-1$       | $+1$       | $(1^2 + (-1)^2 + 1^2)/3 = 1$                        |
| $2$        | $0 $       | $0 $       | $+2$       | $(0 + 0 + 2^2)/3 = 4/3$                             |
| $3$        | $-1$       | $-1$       | $+1$       | $((-1)^2 + (-1)^2 + 1^2)/3 = 1/3$                   |
| $4$        | $-2$       | $0 $       | $0 $       | $((-2)^2 + 0 + 0)/3 = 4/3$                          |
| $5$        | $-1$       | $+1$       | $-1$       | $((-1)^2 + 1^2 + (-1)^2)/3 = 1$                     |


In Python, `n` steps of `N` 1D random walkers are simulated as follows:

```python
import numpy as np
step_length = 1
steps = np.random.choice([-step_length, step_length], size=(N, n))
```
This generates a $n \times N$ matrix of random left/right displacements. Each walker follows its row of displacements.

In the experiments below, you can see that as the number of walkers `N` gets higher, due to the Central Limit Theorem,
the system's properties tend to stabilize. The mean displacement will settle around zero,
and the variance is increasing linearly with the number of steps. The bell shape of the PDF gives us 
a hint that it can be modelled as a Gaussian distribution. 

We can make certain observations/guesses from these measurements. But first, to set up the dotation, let $n$ be the 
number of steps, $d$ be the step length, $\<.\>$ the expectation (mean) operator on some array and $x$ the displacement. 

* The walkers spread away from the starting point in a normal distribution.
* $MSD \propto n$  
* $MSD \propto \ell^2$  
* $\sqrt{Var(x)} \propto d$ (keep in mind that the width of the Gaussian bell is $\approx 6\sqrt{Var(x)}$)
* $\sqrt{Var(x)} \propto \sqrt{n}$ (same remark for the width)
* $\langle x \rangle \approx 0$

Note that since $\langle x\rangle \approx 0$, $MSD$ and $Var(x)$ are effectively the same;

$Var(x) = \langle x^2\rangle - \langle x\rangle^2 = \langle x^2\rangle = MSD$

The code to generate these plots is listed below.

```python
import numpy as np
import matplotlib.pyplot as plt
from math import floor

### Change the parameters below
N = 100  # number of walkers
n = 200  # steps per walker
step_length = 1

steps = np.random.choice([-step_length, step_length], size=(N, n))
positions = np.cumsum(steps, axis=1)

### mean squared distance of all walkers
squared_displacements = positions**2
msd = np.mean(squared_displacements, axis=0)

### average displacement over time
avg_displacement = np.mean(positions, axis=0)

### Displacement histogram (spread of all particles)
min_displacement = positions.min()
max_displacement = positions.max()
num_buckets = N // 4
bins = np.linspace(min_displacement, max_displacement, num_buckets + 1)

hist_sum = np.zeros(num_buckets)
for step in range(n):
    hist, _ = np.histogram(positions[:, step], bins=bins)
    hist_sum += hist
# average histogram for all steps
avg_hist = hist_sum / n
plt.figure(figsize=(20, 5))
plt.suptitle(f"1D random walks for {N} walkers, {n} steps, step length of {step_length}",
    fontsize=14, weight="bold")

### Plots
# 1. random walk trajectories
plt.subplot(1, 4, 1)
plt.grid()
for i in range(N):
    color = np.random.rand(3,)
    # avoid grayish tones 
    color[int(i % 3)] = (int((255*color[0] + 255*color[1]) // 2 + i * 0x82ef16a9) % 255)/255
    if N >= 50:
        alpha = min(floor(N//25)/100, 0.85)
    else:
        alpha = 1
    plt.scatter(range(n), positions[i], color=color, marker="o", s=10,
        alpha=alpha, label=f"Walker {i+1}")
ymin, ymax = plt.ylim()
y_ticks = np.arange(np.floor(ymin), np.ceil(ymax) + 1, (ymax - ymin)//8)  # Integers only
plt.xlabel("step")
plt.ylabel("position")

# 2. average displacement
plt.subplot(1, 4, 2)
plt.grid()
plt.plot(range(n), avg_displacement, color="red", label="Average displacement", linewidth=2)
plt.xlabel("step")
plt.ylabel("Average displacement")
plt.title("Average displacement of walkers")

# 3. MSD over time (middle right)
plt.subplot(1, 4, 3)
plt.grid()
plt.plot(range(n), msd, color="blue", label="MSD", linewidth=2)
plt.xlabel("step")
plt.ylabel("MSD")
plt.title("Mean squared displacement over time")

# 4. average histogram over all steps (right) - the width is $\approx 6  \sqrt{Var(x)}$
plt.subplot(1, 4, 4)
plt.grid()
plt.bar(bins[:-1], avg_hist/(N), width=np.diff(bins), edgecolor="black", align="edge")
plt.xlabel("position")
plt.ylabel("probability")
plt.title(f"Position probability\n(averaged over all steps)")

plt.tight_layout()
plt.savefig(f"{N}_random_walkers_{n}_steps_{step_length}_size.png", dpi=300)
plt.show()
```

<a href="https://github.com/leonmavr/leonmavr.github.io/blob/master/_posts/2024-12-22-DLA/random_walkers_experiment.png?raw=true" target="_blank">
    <img src="https://github.com/leonmavr/leonmavr.github.io/blob/master/_posts/2024-12-22-DLA/random_walkers_experiment.png?raw=true" alt="Click to enlarge" width="800">
</a>

# 2. Random Walk Theory

## 2.1 Rules of Random Walk

Unbiased random walks for a set of particles are microscopically dictated by the following rules:

1. All particles are independent and do not interact.
2. Move on the vertices of a lattice every $\Delta t$ units of time - remaining still is not allowed.
3. When moving, choose a neighboring vertex with equal probability.

[*ref*](https://www2.gwu.edu/~phy21bio/Reading/randomwalkBerg.pdf)

## 2.1 From The Master Equation to The Diffusion Differential Equation 

Start by considering the one dimensional discrete *unbiased* random walk, i.e. the walker always moves to a neighboring cell
with equal probabilities.  Then the equation
that relates the probability $P$ that it's found current position $x$ based on the previous positions and their probabilities is called
**Master Equation**. For an 1D lattice of width $\Delta x$, the Master Equation is expressed as:

$P(x, t) = \dfrac{1}{2}P(x - \Delta x, t - \Delta t) + \dfrac{1}{2}P(x + \Delta x, t - \Delta t)$

This is nice and neat but it only computes the current position based on the previous state. To be 
able to predict the position at any time, we need to transform it to a partial differential equation (PDE).

$P(x, t) = \dfrac{1}{2}P(x - \Delta x, t - \Delta t) + \dfrac{1}{2}P(x + \Delta x, t - \Delta t) \Rightarrow$

$P(x, t) - P(x, t-\Delta t) = \dfrac{1}{2}\Big[P(x - \Delta x, t - \Delta t) - 2P(x,t-\Delta t) + P(x + \Delta x, t - \Delta t)\Big]$

If we look at Eq. (**TODO**) and (**TODO**) in A.1, the LHS is equal to $P_t(x,t)\Delta t$. The RHS is $\frac{P_{xx}(x,t)\Delta x}{2}$
Therefore for the unbiased random walk, we end up with the equation:

$P_t(x,t)\Delta t = \dfrac{P_{xx}(x,t)\Delta x}{2} \Rightarrow P_t(x,t) = \underbrace{\dfrac{\Delta x}{2\Delta t}}\_{D} P_{xx}(x,t)$

The *diffusion coefficient* $D$ expresses how fast the walkers tend to spread.

In general however, the denominator depends
on the degrees of freedom $N$. For the 1D case, $N$ is obviously $1$. For the 2D case with a 4-neighborhood, $N=2$ and
for the 2D case with an 8-neighborhood, $N=4$. For the 3D motion with a 6-neighborhood, $N=3$, etc. So the general
differential equation for the unbiased random walk is:

$\boxed{P_t(x,t) = \dfrac{\Delta x}{2N\Delta t} P_{xx}(x,t)}$

This PDE holds for *a system of walkers*. Therefore if we solve it, we will end up finding the PDE of the system,
i.e. what's the probability of finding a walker at each point in space.

## 2.2 First and Second Moments of Random Walks

[ref](https://www2.gwu.edu/~phy21bio/Reading/randomwalkBerg.pdf)

### 2.2.1 First Moment (Mean)

We define the first moment $\langle x \rangle$(a.k.a. expected value or mean) of some series $x(n)$ as:

$\langle x \rangle = \dfrac{1}{N}\sum\limits_{n=1}^{N}x(n)$

To derive the statistics of random walks, we discretize the time as $n:= \frac{t}{\Delta t}$ in order the simplify
the notation and $n$ is measured in steps instead of time units. Then for the $i$-th particle, the position after 
$n$ step differs from the one after $n-1$ steps by $\Delta x_n = \pm \Delta x$:

$x_i(n) = x_i(n-1) + \Delta x_n$

As $n$ gets larger and from the Central Limit Theorem, half of the displacments will be to the right and half to the left,
i.e. $\langle \Delta x_n \rangle = 0$. Taking expectations in the latter equation:

$\langle x_i(n) \rangle = \langle x_i(n-1) + \Delta x_n \rangle = \langle x_i(n-1) \rangle + \underbrace{\langle \Delta x_n \rangle}\_{0}$

The latter equation tells us that the mean does not change from step to step. Consequently, the mean is the same
as the initial mean. If the particles start from the origin, then $\langle x_i(n) \rangle = 0$

### 2.2.2 Second Moment (Variance)

Variance measures how much $x(n)$ deviates from its mean, i.e. how much the particles spread. It's defined as:

$Var(x) = \langle \left(x - \langle x \rangle \right)^2\rangle$

Expanding the variance for the random walk $x(n)$:

$Var(x) = \langle x^2 - 2x\langle x \rangle + \langle x \rangle^2\rangle$

$ = \langle x \rangle^2 - 2\langle x \rangle \langle x \rangle + \langle x \rangle^2$

$ = \langle x^2 \rangle - \langle x \rangle^2$

$\langle x^2 \rangle$ is the average of the squared values and $\langle x^2 \rangle$  is the square of the mean.
For the 1D random we can compute $x^2_i(n)$ from Eq. (**TODO**):

$x_i^2(n) = x_i^2(n-1) + \Delta x_n x_i(n-1) + \Delta x_n^2$

Applying the expectation:

$\langle x_i^2(n) \rangle = \langle x_i^2(n-1) \rangle + 2 \langle \Delta x_n x_i(n-1) \rangle + \langle \Delta x_n^2 \rangle$
$ = \langle x_i^2(n-1) \rangle +  \Delta x^2 $

Recurrently, we begin with $n=1$ and find $\langle x_i^2(1) \rangle = \Delta x^2$, then $\langle x_i^2(2) \rangle = 2\Delta x^2$ and by induction
$\langle x_i^2(n) \rangle = n \Delta x^2$. This confirms the observation about the square root of the variance in Section **TODO**.

If we transition to continuous time domain, we can find how the variance evolves over time. Substituting $n = t/ \Delta t$:

$\langle x_i^2(t) \rangle = \frac{t\Delta x^2}{\Delta t} = 2D t$

Or alternatively,

$\sqrt{\langle x_i^2(t) \rangle} = \sqrt{ 2D t}$

, where $D = \frac{\Delta x^2}{2 \Delta t}$ is the diffusion coefficient as defined in Section 2.1.
So the root mean squared displacement (RMSD) is proportional to the square root of time, as observed in Section 1.
This says that the particles spread in a pulse whose height linearly decreases with time and whose are is conserved,
till it's flattened. Because is a visualization of Gaussians with variance of $\sqrt{2D t}$


<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/gaussian_spread.png" alt="" width="500">


## 2.3 Meaning and Solution of the Diffusion PDE 

### 2.3.1 Why Is the Solution a Gaussian

We'll try to figure out the density of the particles as they drift away from the starting position.

First, consider once again the unbiased 1D random walker from time $t=0$ to $t=4$. 
If use use the master equation to compute the current position $x(t)$, we end 
up for the following probabities $P(x,t)$ for every point in time $t$. 
You can see at the bottom row that the profile of the average sum of position probablities
already starts to resemble with a bell shape.

|$x$       | $-4$   | $-3$   | $-2$   | $-1$   |$0$    |  $1$  |  $2$  | $3$   |    $4$ |
|----------|-------:|-------:|-------:|-------:|------:|------:|------:|------:|-------:|
| $t=0$    |        |        |        |        | 1     |       |       |       |        |
| $t=1$    |        |        |        |  0.5   |       | 0.5   |       |       |        |
| $t=2$    |        |        |   0.25 |        | 0.25  |       |  0.25 |       |        |
| $t=3$    |        |  0.125 |        |  0.125 |       | 0.125 |       | 0.125 |        |
| $t=4$    | 0.0625 |        |  0.0625|        |0.0625 |       | 0.0625|       | 0.0625 |
| Avg. sum | 0.0125 |  0.025 | 0.0625 |  0.125 |0.2625 | 0.125 |0.0625 | 0.025 | 0.0125 |

Now consider a biased random walker with moving probabity of $p$ to the left and $q$ to the right ($p+q=1$).
Because the particle can either move left or right, the probability of it stepping $k$ times to the left in 
$n$ trials ($k\leq n)$ is given by:

$P(k;n,p) = \frac{n!}{k!(n-k)!}p^kq^{n-k}$

If $d$ is the step length, then the total displacement is the displacement to the right minus the displacement to the left:

$x(n) = [(1-k) - (n-(1-k))]d = (2k-n)d$

$k$ follows a binomial distribution and $x$ is linearly related with $x$. For a binomial distribution,
we know 

$\langle k \rangle = nq$

$Var(k) = npq$

We know that as $n$ gets large, the distribution of $k$ will resemble a Gaussian one (I'll show why soon).
From the linearity of $<.>$ and the property $Var(aX + b) = a^2Var(X)$, we obtain for the distribution of $x$:

$\langle x \rangle = \langle (2k-n)d \rangle = (2\langle k \rangle -n)d = (2q-1)nd$

$Var(x) = Var\Big((2k-n)d\Big) = 4d^2 Var(k) = 4d^2 npq$

Let's see why the distribution of $k$ and hence of $x$ will resemble with a Gaussian one.

[*ref*](https://www.mv.helsinki.fi/home/knordlun/mc/mc7nc.pdf)

To do this, consider once again a random walk with $n$ steps, step length $d$, and time $\tau$
between steps. We'll keep track of the following variables:

* $n = \frac{t}{\tau}$: number of steps at time $t$
* $m = \frac{x}{d}$: number of steps taken to the **right** given that the particle is as position $x$.

It's obvious that if the particle is found at $x$, then it's taken exactly $\frac{1}{2}(m+n)$ steps to
the right and $\frac{1}{2}(n-m)$ steps to the left, summing up to $n$ total steps. For each trial ($n$),
we compute the probability of each number of steps to the right ($\frac{1}{2}(m+n)$ from $0$ to $n$).

To deduce every possible path leading to each $m$, recall that $\frac{1}{2}(m+n)$ must be a whole number,
which is true only if both $n$ and $m$ are either odd or even. $m$ is limited to $[-n, n]$.

The figure below outlines all possible walks for every number of steps $n$ and every number of steps to 
the right $\frac{1}{2}(m+n)$. For probability of each sequence, $p_n(m) = \left(\frac{1}{2}\right)^m \cdot (number \; of \; sequences \; leading \; to \; m )$.
, where $number \; of \; sequences \; leading \; to \; m = \frac{total \; permutations}{repetitions \; of \; identical \; steps}$.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/possible_walks_arrows.png" alt="" width="700">

By observing at the results in the column on the right, one can deduce that the formula for the probability 
$p_n(m)$ is:

$P_n(m) = \left(\frac{1}{2}\right)^n \frac{n!}{\left[\frac{1}{2}(n+m)!\right] \left[\frac{1}{2}(n-m)!\right]}$

But *why* does this formula work?

To answer this, consider the random walk as a set of $n$ permutable left and right arrows.
Let's consider the case of step $n=4$, position $m=2$. The walker must have taken 3 steps to the right ($\rightarrow$)
and 1 to the left ($\leftarrow$). The number of distinct ways to reach position $\frac{1}{2}(m+n)$, the possible arrangements
of steps are the following, hence $p_n(m) = 4$:

1. $\rightarrow$, $\rightarrow$, $\rightarrow$, $\leftarrow$
2. $\rightarrow$, $\rightarrow$, $\leftarrow$, $\rightarrow$
3. $\rightarrow$, $\leftarrow$, $\rightarrow$, $\rightarrow$
4. $\leftarrow$, $\rightarrow$, $\rightarrow$, $\rightarrow$

The numerator of the $p_n(m)$ formula is straightforward - it's just the total number of arrow permutations, i.e. $n!$. 
The denominator should contains all arrows arrangements that lead to the same position $m=2$.
Think about the trajectory as $n=4$ distinct slots
and each arrow as a separate entity. Therefore to differentiate the 3 right arrows, label them as $\rightarrow_1$, $\rightarrow_2$,
$\rightarrow_3$. There's a sole left arrow that doesn't need to be labelled.

We want to find *how many combinations* of $\rightarrow_1$, $\rightarrow_2$, $\rightarrow_3$ ($\leftarrow$ is omitted) lead 
to the same final position and the answer is $3!$. If we arrange them by picking $\rightarrow_1$, then we can pick  either
$\rightarrow_2$ or $\rightarrow_3$, and the non-picked $\rightarrow_2$ or $\rightarrow_3$ is fixed. That's $2\cdot 1$ combinations 
for $\rightarrow_1$. If we cycle the indices starting from $\rightarrow_2$, and $\rightarrow_3$, that's $3\cdot 2\cdot 1 = 3!$ total 
combinations listed below:

<table style="border: none; border-collapse: collapse;">
<tr>
<td style="border: none;">
1. $\rightarrow_1 \rightarrow_2 \rightarrow_3$ <br>
2. $\rightarrow_1 \rightarrow_3 \rightarrow_2$ <br>  
3. $\rightarrow_2 \rightarrow_1 \rightarrow_3$ <br>
</td>
<td style="border: none;">
4. $\rightarrow_2 \rightarrow_3 \rightarrow_1$ <br>
5. $\rightarrow_3 \rightarrow_1 \rightarrow_2$ <br>
6. $\rightarrow_3 \rightarrow_2 \rightarrow_1$ <br>
</td>
</tr>
</table>

That's exactly $[(m+n)/2]! \cdot [(n-m)/2]! = 3! \cdot 1!$ in the denominator.
Using the same rationale, one can verify the denominator for larger paths, e.g. for $n=5$ and $(m+n)/2=1$
(3 steps to the right, 2 to the left). 

Now going back to the $p_n(m)$ formula as $n$ gets large:

$P_n(m) = \left(\frac{1}{2}\right)^n \frac{n!}{\left[\frac{1}{2}(n+m)\right]!\left[\frac{1}{2}(n-m)\right]!}$

This can be approximated approximated by continuous functions by getting rid of the factorial. Using Stirling's 
approximation:

$N! \approx \sqrt{2\pi N} \left( \frac{N}{e} \right)^N \Rightarrow$

$\ln (N!) \approx N \ln N - N + \frac{1}{2}\ln (2\pi N)$

Taking the $\ln$ of both side in the expression for $p_n(m)$ and after some tedious algebra, it turns out that
$p_n(m)$ can be approximated as a Gaussian:

$p_n(m) \approx \frac{2}{\sqrt{2\pi n}} e^{-\frac{m^2}{2n}}$

Substituting in terms of the continuous variables; $m = x/\Delta x$, $n = t/\Delta t$

$p_n(m) = \frac{1}{\sqrt{2\pi \frac{t}{\Delta t}}} \exp\left(-\frac{x^2}{\Delta x^2 2 \frac{t}{\Delta t}}\right)$

$= \frac{\sqrt{\Delta t}}{\sqrt{2\pi}t} \exp\left(-\frac{x^2 \Delta t}{2\Delta x^2 t}\right)$

By letting the diffusion coefficient $D$ be $D:=\frac{\Delta x^2}{2\Delta t}$ then the solution 
$p_n(m)$ is written as

$p_n(m) = \frac{\Delta x}{2 \sqrt{\pi t D}} \exp\left(-\frac{x^2}{4D} \right)$

Comparing this with the stanrdard form of the Gaussian, which is

$\mathcal{N}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\langle x \rangle)^2}{2\sigma^2} \right)$

We obtain

$\sqrt{2\pi \sigma^2} = \frac{2\sqrt{\pi t D}}{\Delta x} \Rightarrow \sigma^2 = \frac{2Dt}{(\Delta x)^2} \Rightarrow \sigma = \sqrt{2Dt}$

This is the same standard deviation $\sigma$ as the one obtained by manipulating the diffusion PDE.




## 2.4 Biased Random Walks

For the biased walk case, suppose that at every step the particle may step left at $x-\Delta x$ with
probabity $p$ and right at $x + \Delta x$ with probabity $q$ ($p + q = 1$). Then the Master Equation is written:

$P(x, t + \Delta t) = pP(x - \Delta x, t) + qP(x + \Delta x, t)$

Let $\Delta x \rightarrow 0$ and $\Delta t \rightarrow 0$, then we can use the finite difference method to 
approximate $P(x - \Delta x, t)$ and $P(x + \Delta x, t)$ around point $(x,t)$. Alternatively, you can just use Taylor
series, however I'll stick to finite differences to keep it in theme with the unbiased case derivation.

From the finite difference definitions of the first and second derivatives:

$P(x+\Delta x, t) - P(x-\Delta x, t) \approx 2 \Delta x P_x(x,t)$
$P(x+\Delta x, t) + P(x-\Delta x, t) \approx 2 P(x, t) + \Delta x^2 P_{xx}(x,t) $

Summing by parts and then subtracting by parts, for each displacement $\Delta x$ and $-\Delta x$:

$P(x \pm \Delta x, t) \approx P(x,t) \pm \Delta x P_x(x,t) + \frac{\Delta x^2}{2} P_{xx}(x, t)$

Then we can expand the RHS of the Master Equation around $(x\pm \Delta x, t)$ as:

$P(x, t+ \Delta t) = p\left(P(x,t) - \Delta x P_x(x,t) + \frac{\Delta x^2}{2} P_{xx}(x, t)\right) +$
$\quad \quad \quad \quad \quad \quad \; \ q\left(P(x,t) + \Delta x P_x(x,t) + \frac{\Delta x^2}{2} P_{xx}(x, t)\right)$
$\quad \quad \quad \quad \quad \; \ =  P(x,t) + \Delta x (p-q) P_x(x,t) + \frac{\Delta x^2}{2} P_{xx}(x, t) \Rightarrow$

$P(x, t + \Delta t) - P(x,t) = \Delta x (q-p) P_x(x,t) + \frac{\Delta x^2}{2} P_{xx}(x, t)$

However, $P(x, t + \Delta t) - P(x,t) \approx P_t(x,t) \Delta t$ so we end with the following partial
differential equation:

$ \boxed{P_t(x,t) = \underbrace{\frac{\Delta x (q-p)}{\Delta t}}\_{u} P_x(x,t) + \underbrace{\frac{\Delta x^2}{2\Delta t}}\_{D} P_{xx}(x, t) }$

Two constants appear - the *drift velocity* $u$ and the *diffusion coefficient* $D$.

* $u$ is the velocity of a flow that carries the distribution $P(x,t)$ to the right over time if $u>0$, else to the left.
You can think of it as the average speed that carries the system of particles -- e.g. the speed of a wind carrying pollen.
* $D$ is  determines the rate at which $P(x,t)$ spreads outward -- think of it as the random spreading of pollen
floating still in the air.

[TODO](https://rosdok.uni-rostock.de/file/rosdok_derivate_0000003390/Skript_SP_SS2006.pdf)

The latter is an 1D case of the **diffusion-advection equation**, also known as **Fokker-Planck equation**.
The 2D and 3D cases involve the same terms with the same derivatives, albeit in more dimensions.

In 2D and 3D with a drift speed $\textbf{u} = (u_x, u_y)$, $\textbf{u} = (u_x, u_y, u_z)$ respectively, we can write:

$P_t + u_xP_x + u_y P_y = D\left(P_{xx} + P_{yy} \right) \leftrightsquigarrow P_t + \textbf{u} \cdot \nabla P = D \nabla^2 P$

$P_t + u_xP_x + u_y P_y + u_z P_z= D\left(P_{xx} + P_{yy} + P_{zz} \right) \leftrightsquigarrow P_t + \textbf{u} \cdot \nabla P = D \nabla^2 P$

* $\nabla P = (P_x, P_y, P_z)$ is the spatial gradient of scalar $P(x, y, z, t)$, and points to the steepest ascent of $P(x,y,z,t)$ in the $x,y,z$ space.
* $\textbf{u} \cdot \nabla P = u_xP_x + u_yP_y + u_zP_z$ is rate of change of $\nabla P$ along its projection on vector $u$.
* $\nabla^2 P = P_{xx} + P_{yy} + P_{zz}$ is the spatial Laplacian of scalar $P(x,y,z,t)$, indicating how much $P$ deviates from
its average value in each neighborhood of $(x,y,z)$. You can think of it as how how many peaks and valleys $P(x,y,z)$ has and how deep they are.
If it's turbulent then the magnitude of the Laplacian is high. If $P(x,y,z)$ is smooth, then the magnitude is low.

<table border="1" style="width: 100%; border-collapse: collapse;">
    <tr>
        <td style="text-align: center; vertical-align: middle;">
            <a href="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(0.5x)_sin(0.5y).png" target="_blank">
                <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(0.5x)_sin(0.5y).png" alt="Image 1" style="max-width: 100%; max-height: 100%;">
            </a>
        </td>
        <td style="text-align: center; vertical-align: middle;">
            <a href="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(1x)_sin(1y).png" target="_blank">
                <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(1x)_sin(1y).png" alt="Image 2" style="max-width: 100%; max-height: 100%;">
            </a>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle;">
            <a href="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(2x)_sin(2y).png" target="_blank">
                <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(2x)_sin(2y).png" alt="Image 3" style="max-width: 100%; max-height: 100%;">
            </a>
        </td>
        <td style="text-align: center; vertical-align: middle;">
            <a href="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(4x)_sin(4y).png" target="_blank">
                <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lapl_sin(4x)_sin(4y).png" alt="Image 4" style="max-width: 100%; max-height: 100%;">
            </a>
        </td>
    </tr>
</table>

## 2.5 Solving the Diffusion-Advection Equation

### 2.5.1 Solution via Fourier transform

In the generalized (biased) case of 1D random walks, we arrived at the diffusion-advection equation 
that describe simultaneous transform (with scalar velocity $u$) and diffusion (with coefficient $D$):

$\frac{\partial P}{\partial t} + u \frac{\partial P}{\partial x} = D \frac{\partial^2 P}{\partial x^2}$

To solve it, we will apply Fourier transform, which has the following definition and properties:

* Fourier transform of $P(x,t)$ w.r.t $x$: $\mathcal{F}[P(x, t)] = \hat{P}(k,t) = \int_{-\infty}^{\infty} P(x,t) e^{-ikx}dx$
* $\mathcal{F}\left[\frac{\partial P}{\partial x}\right] = ik\hat{P}(k,t)$
* $\mathcal{F}\left[\frac{\partial^2 P}{\partial x^2}\right] = -k^2\hat{P}(k,t)$

Additionally use:

* $\mathcal{F}\left[\frac{\partial P}{\partial t} \right] = \frac{\partial \hat{P}}{\partial t}$

Applying these, the 1D diffusion-advection equation becomes:

$\frac{\partial \hat{P}}{\partial t} + iku\hat{P}(k,t) = -Dk^2\hat{P}(k,t)$

This can be treated as a 1st order ODE w.r.t $\hat{P}$:

$\frac{\partial \hat{P}}{\partial t} + iku\hat{P}(k,t) = -Dk^2\hat{P}(k,t) \Rightarrow$

$\frac{\partial \hat{P}}{\partial t} =  -\underbrace{(iku+Dk^2)}_{\alpha(k)}\hat{P}(k,t)$

The solution is $\hat{P} = \hat{P}(k,0)e^{-\alpha(k)t}$ and by substituting $\alpha(k) = iku+Dk^2$:

$\hat{u}(k,t) = \hat{P}(k, 0)e^{-(iku + Dk^2)t}$

Now to return from the frequency space to the time space, we apply the inverse Fourier transform:

$u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{P}(k,t)e^{ikx}dk$

Substituting the solution for $\hat{u}(k,t)$:

$u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{P}(k, 0)e^{-(iku + Dk^2)t}e^{ikx}dk$

$u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{P}(k, 0)e^{ik(x - ut)}e^{-Dk^2t}dk$

Before evaluating the solution, we need to find the Fourier transform $\hat{u}(k,0)$ of the initial condition $u(x,0)$.
The initial condition tells us what the spatial distribution $f(x)$ is at $t=0$, i.e. $P(x,0) = f(x)$.

$\hat{P}(k, 0) = \mathcal{F}[f(x)] = \int_{-\infty}^{\infty}f(x)e^{-ikx}dx$

Substitute $\hat{P}(k, 0)$ into the expression for the solution $u(x,t)$:

$u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\left( \int_{-\infty}^{\infty}f(\xi)e^{-ik\xi}d\xi \right)\, e^{ik(x - ut)}e^{-Dk^2t}dk$

$\frac{1}{2\pi} \int\limits_{-\infty}^{\infty}f(\xi)\left(\int_{-\infty}^{\infty} e^{ik(x-ut-\xi)}e^{-Dk^2t}dk\right) d\xi$

The inner integral is Gaussian integral of $k$:

$\int_{-\infty}^{\infty}e^{ik(x-ut-\xi)}e^{-Dk^2t}dk = \sqrt{\frac{\pi}{Dt}}\exp\left(-\frac{(x-ut-\xi)^2}{4Dt} \right)$

Substituting this into the expression for the solution $P(x,t)$:

$\boxed{P(x,t) = \frac{1}{\sqrt{4\pi D t}} \int_{-\infty}^{\infty}f(\xi) e^{-\frac{(x-ut-\xi)}{4Dt}} d\xi}$

Recall that $f(x) = P(x,t=0)$ is the initial condition.


### 2.5.2 Solution For Various Initial Conditions

We consider two cases for the initial condition $f(x) := P(x,t=0)$.

#### 1. Single point source  

Then $f(\xi) = c\delta(\xi)$, where $c$ is a constant and $\delta(\xi)$ is the Dirac delta function.
However, we let $c=1$ to normalize $f(\xi)$, i.e. $f(\xi) = \delta(\xi)$, where

$
\delta(\xi) =
$
$$
\begin{cases}
\infty \; &, \; \xi = 0 \\
0 \; &, \; \xi \neq 0 
\end{cases}
$$

such that 

$\int_{\infty}^{\infty}\delta(\xi)d\xi = 1$

Then the solution $P(x,t)$ is:

$P(x,t) = \frac{1}{\sqrt{4\pi D t}} e^{\frac{ut-x}{4Dt}}$


#### 2. Uniform random spread

If the particles are evenly spread along an interval $[-L,L]$

$
f(\xi) =
$
$$
\begin{cases}
\frac{1}{2L} \; &, \; -L \leq \xi \leq L \\
0 \; &, \; \text{otherwise} 
\end{cases}
$$

Then plugging in that $f(\xi)$ and since $-L\leq \xi \leq L$:

$P(x,t) = \dfrac{1}{\sqrt{4\pi Dt}} \int_{-L}^{L} \frac{1}{2L}e^{-\frac{(x-\xi-ut)}{4Dt}}d\xi$
$=\dfrac{1}{2L\sqrt{4\pi Dt}} \int_{-L}^{L} e^{-\frac{(x-\xi-ut)}{4Dt}}d\xi$

The exponential integral cannot be computed analytically and requires computational methods.

### 2.5.3 $d$-dimensional Solution

Let $\textbf{u} = (u_1, u_2, \ldots, u_d)$ be the advection velocity vector and 
$\textbf{x} = (x_1, x_2, \ldots, x_d)$ be the position in the $d$-dimensional space. The diffusion-advection 
equation in multiple dimensions is written as:

$\frac{\partial P(x,t)}{\partial t} + \textbf{u}\cdot \nabla P(x,t) = D \nabla^2 P(x,t)$

We can once again solve it with the Fourier transform:

$\hat{P}(\textbf{k},t) = \mathcal{F}[P(\textbf{x},t)] = $
$\int_{\mathbb{R}^d}P(\textbf{x},t)e^{-i\textbf{k}\cdot \textbf{x}}d\textbf{x}$

, by recalling the properties:

* $\mathcal{F}\left[\nabla P(x,t) \right] = i\textbf{k}\hat{P}(\textbf{k}, t)$
* $\mathcal{F}\left[\nabla^2 P(x,t) \right] = -\left\|\textbf{k}\right\|^2\hat{P}(\textbf{k}, t)$

Taking the Fourier of the $(\textbf{x}, t)$ diffusion-advection equation:

$\frac{\partial \hat{P}}{\partial t} + i(\textbf{u}\cdot \textbf{k}) \hat{P}(\textbf{k}, t) = -D\left\|k \right\|^2\hat{P}(\textbf{k},t) \Rightarrow$

$\frac{\partial \hat{P}}{\partial t} = -\left(D\left\|k \right\|^2 + i(\textbf{u}\cdot \textbf{k}) \right)\hat{P}(\textbf{k},t)$

This is a linear 1st order ODE for $\hat{P}(\textbf{k},t)$ with respect to $t$. Its solution is:

$\hat{P}(\textbf{k},t) = \hat{P}(\textbf{k},0) \exp\left(-\left(i(\textbf{u}\cdot \textbf{k}) + D\left\|\textbf{k} \right\|^2) \right)t\right)$

To return to the spatial domain, we take the inverse Fourier transform of $\hat{P}(\textbf{k},t)$:

$\boxed{P(\textbf{x},t) = \frac{1}{(2\pi)^d}\int_{\mathbb{R}^d} \hat{P}(\textbf{k},0) \exp\left(-\left(i(\textbf{u}\cdot \textbf{k}) + D\left\|\textbf{k} \right\|^2) \right)t\right) \exp(i\textbf{k}\cdot \textbf{x}) d\textbf{k}}$

The solution consists of several components:

1. $\exp(-i(\textbf{u}\cdot \textbf{k}))$ represents the displacement of the solution due to advection with velocity $\textbf{u}$.
2. $\exp(-D\left\|k\right\|^2t)$ represents the diffusion over time with constant $D$.
3. $\hat{P}(\textbf{k},t)$ 
4. $\exp(i\textbf{k}\cdot\textbf{x}) = \cos(\textbf{k}\cdot\textbf{x}) + i\sin(\textbf{k}\cdot\textbf{x})$ is just a planar wave of 
frequency $\left|\textbf{k}\right|$ and direction $\textbf{k}$ and acts as a basis function for decomposing $P(\textbf{x},t)$ into components of 
different frequencies.

Note that the general solution can be written in a manner analogous to the 1D solution as:

$P(\textbf{x},t) = \frac{1}{(2\pi)^d}\int_{\mathbb{R}^d}\hat{P}(\textbf{k},0)\exp(i\textbf{k}\cdot(\textbf{x}-\textbf{u}t)) \exp(-D\left\|\textbf{k} \right\|^2t) d\textbf{k}$

## 2.6 Macroscopic Point of View and Fick's Laws 

So far we've studied diffusion from a microscopic point of view via as a set of random walks.
We have arrived to the same conclusions by viewing the phenomenom from a macroscopic point of view.

Fick's laws express diffusion as the flux (flow) of matter and the change of concentration over time.
To derive them, we consider the *convervation of mass* as an axiom and consider the case of 1D diffusion
along a very thin line (like a straight segment of a hair).

Consider the case of transferring the material along this line in the absense of any chemical reactions.
The two quantities we use to describe the flow are:

* the concentration; how many mols there are per unit volume; $C(x,t)$ in $\frac{mol}{s\cdot cm^3}$
* the flux; net mass transfer amount per unit time per area; $J(x,t)$ in $\frac{mol}{s\cdot cm^2}$

### 2.6.1. Fick's First Law

Fick's first law states *how particles move in space* - particularly that the flux is proportional to
the concentration of gradient $\tfrac{dC}{dx}$. It states that the particles flow from high concentration to low concentration:

$J(x,t) = -D \dfrac{\partial C(x,t)}{\partial x} \leftrightsquigarrow J(x,t) = -D \nabla C$

* $J$ is the flux $\left[\frac{mol}{m^2\cdot s} \right]$
* $D$ is the diffusion coefficient $\left[ \frac{m}{s^2}\right]$ 
* $\nabla C$ is the concentration gradient $\left[ \frac{mol}{m^4}\right]$ 

It can be derived in 1D by examining the microscopic motion of particles through the membrane (dashed line)
of a rectangular  cross section of area $A$ during some duration $\Delta t$.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/ficks_first_rectangle.png" alt="" width="500">

The particles moves at equal probability towards each direction ($N(x+\Delta x)$ mols move to the left, 
$N(x)$ to the right) so from the definition of flux:

$J(x,t) = \dfrac{1}{A}\dfrac{\frac{N(x)}{2} - \frac{N(x+\Delta x)}{2}}{\Delta t}$

However, $N(x) = C(x,t)A\Delta x$ therefore $J(x,t)$ equals to

$J(x,t) = \dfrac{1}{2A}\dfrac{C(x,t)A\Delta x - C(x+\Delta x,t)A\Delta x}{\Delta t}$

$\qquad \;\;\;\;= -\dfrac{\Delta x^2}{2\Delta t}\dfrac{C(x+\Delta x,t) - C(x,t)}{\Delta x} \Rightarrow$

$\boxed{J(x,t) = -D \dfrac{\partial C(x,t)}{\partial x}\ , \quad D = \dfrac{\Delta x^2}{2\Delta t}}$

### 2.6.2. Fick's Second Law

Fick's second law describes how the *concentration of a substance changes over time* due to diffusion.
Particularly, it states that a change in concentration $C(x,t)$ over time is given by the difference between
the flux in and flux out of an element of width $\Delta x$:

$\dfrac{\partial C(x,t)}{\partial t} = D \dfrac{\partial^2 C(x,t)}{\partial x^2} \leftrightsquigarrow \dfrac{\partial C}{\partial t} = D \nabla^2 C(x,t)$

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/ficks_second.png" alt="" width="500">

The second law is derived from the first one and the conservation of mass. We will derive it for
a horizontal infiniteseminal element of width $\Delta x \rightarrow 0$.

Recall Fick's first law:

$J(x,t) = -D\dfrac{\partial C(x,t)}{\partial x}$

From the conservation of mass:

$\dfrac{\partial C(x,t)}{\partial t} = \dfrac{J(x,t) - J(x+\Delta x, t)}{\Delta x}$

$\qquad \quad \; \; \; \;  = -\dfrac{\partial J(x,t)}{\partial x}$

Differentiate the first law to obtain $J_x(x,t)$. Since in most diffusion systems 
$D$ changes extremely slowly or not at all, we consider it as a constant, therefore:

$\dfrac{\partial J(x,t)}{\partial x} = - \dfrac{\partial }{\partial x} \left(D\dfrac{\partial C(x,t)}{\partial x}\right)$
$=D \dfrac{\partial^2 C(x,t)}{\partial x^2}$

Plugging this in the convservation of mass we obtain the second law:

$\boxed{\dfrac{\partial C(x,t)}{\partial t} = D\dfrac{\partial^2 C(x,t)}{\partial x^2}}$

Fick's second law doesn't only apply to Cartersian coordinates. It can be adapted 
depending on the symmetry of the problem. The table below describes the variables and 
Laplacian operator for various geometries.


<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/laplacian_geometries.png" alt="" width="700">

*ref*: the mathematics of diffusion, clarendon


#### Example

If the concentration of some substance submerged in a an diffusive environment is given at time $t=0$ by a sinusoid:

$C(x,0) = A \sin(kx), \quad k=\dfrac{2\pi}{\lambda}$

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/sine_xy.png" alt="" width="275">

Find out how it will evolve over time.

#### Solution

The zero-concentration nodes will obviously not change and we can assume that the shape of the
rest of the sinudoid will change over time, so the concentration evolution can be expressed as:

$C(x,t) = f(t) C(x,0) = f(t) \sin(kx)$

To determine $f(t)$, plug in this expression in Fick's second law:

$C_t = DC_{xx} \Rightarrow$

$\dfrac{\partial}{\partial t}\Big(f(t) A \sin(kx) \Big) = \dfrac{\partial^2}{\partial x^2} \Big(f(t) A \sin(kx) \Big) \Rightarrow$

$\dot{f}(t) \sin{kx} = -Dk^2f(t) \sin(kx) \Rightarrow$

$\dot{f}(t) = -Dk^2 f(t)$

This is a first order ODE with solution

$f(t) = f(0) e^{-Dk^2t}$

$f(0)$ can be obtained from $C(x,t) = f(t)A\sin(kx)$ for $(x,t) = (\tfrac{\pi}{2k}, 0)$:

$A = f(0) \cdot 1$

$\therefore \; f(t) = Ae^{-Dk^2t}, \quad C(x,t) = f(t)\sin(kx) = Ae^{-Dk^2t}\sin(kx)$

The solution is written w.r.t. the wavelength $\lambda$ as

$\boxed{C(x,t) = Ae^{-Dk^2t}\sin\left(\dfrac{2\pi}{\lambda}x\right)}$

If we look at the first peak for $x=\tfrac{\lambda}{4}$ with concentration

$C(\lambda/4,t) = Ae^{-Dk^2t}$

Then we can determine how the the peak's half life $t_{1/2}$, the time to reach amplitude $A/4$, the the time for 
$A/8$:

$A/2 = Ae^{-Dk^2t_{1/2}} \Rightarrow $

$t_{1/2} = \dfrac{\ln 2}{Dk^2}, \; t_{1/4} = \dfrac{2\ln 2}{Dk^2} = 2t_{1/2}, \; t_{1/8} = 3t_{1/2} \; \therefore \ t_{1/2^n} = \dfrac{n\ln 2}{Dk^2}$

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/sine_diffusion.png" alt="" width="325">


Ref: [1](https://www.doitpoms.ac.uk/tlplib/diffusion/fick2_derivation.php)
[2](https://www-eng.lbl.gov/~shuman/NEXT/MATERIALS&COMPONENTS/Xe_damage/ficks2ndlaw.pdf)
[3](https://ocw.snu.ac.kr/sites/default/files/NOTE/10%20week.pdf)

# 3. Aggregation and Fractal Growth

## 3.1 Aggregation Boundary Conditions

In DLA systems, there are two regions that assert boundary conditions - the cluster and the boundary (wall).
The cluster consists of several particles and if a particle is close enough to the cluster, it remains still 
permanently. The wall can either (a) absorb particles (essentially ''vanish'' them, so they're never considered
in the system again), or (b) reflect  them (bounce them back, hence the next flux there is zero).

1. *sticky cluster*: $C(x_{cl}, t) = 0$ 
2. *at the wall*: (a) $C(x_w, t) = 0$ or (b) $J(x\_{w},t) = 0 \Rightarrow \nabla C(x=x\_w,t) = 0$ (Fick's first law)

Note that $x_{cl}$ changes over time, depending on how many particles are in the cluster.

As the particles arrive and stick to the new boundary, the structure grows over time This biases aggregation towards
the outermost tips, giving rise to the fractal growth we will see in DLA.

## 3.2 First-Passage Time

### 3.2.1 First-Passage PDF

[*1*](https://www.bsmath.hu/15summer/FirstPassageCh1.pdf) [2](https://galton.uchicago.edu/~lalley/Courses/312/RW.pdf) 
[2](https://www.diva-portal.org/smash/get/diva2:1360566/FULLTEXT01.pdf)

The **first-passage time** describes how long it takes a walker to reach the absorbing boundary (e.g. a growing cluster).
For simplicity and because the main goal is to get an idea how fast each particles reaches the cluster, to find the first-passage time we 
only consider the cluster's boundary condition, i.e. its stickiness ($C(x_{cl}, t) = 0$).

The first-passage time probabity is defined via the **survival probabity**. The survival probability $S(t)$
is defined as the total probability that the walker is still inside the domain at time $t$, meaning it has
not yet reached the absorbing boundary (e.g. the cluster):

$S(t) = \int_{x\_{min}}^{x\_{max}}P(x,t) dx$

$P(x,t)$ is the PDF of each particle defined via the diffusion equation for unbiased random walking or via 
the diffusion-advection equation in the biased case. $x_{min}$ and $x_{max}$ define the region of allowed motion
and in case of no external wall and a sticky cluster:

$S(t) = \int_{-\infty}^{x\_{cl}}P(x,t) dx$

Recall that in unbiased random walks, $P(x,t)$ is the solution of 

$\dfrac{\partial P(x,t)}{\partial t} = D \nabla^2 P(x,t)$

Since $S(t)$ is the probabity that a particle is still surviving, the rate at which probabity leaves the system 
is $-\tfrac{dS}{dt}$. This is the first-passage time PDF $F(t)$ for the system:

$\boxed{F(t) = -\dfrac{dS}{dt}}$

In other words, $S(t)$ represents the fraction of particles that are still surviving (unabsorbed) and the negated rate of 
decrease of $S(t)$, i.e. how many particles die at a time $t$ ($-dS/dt$) is the first-passage time for the system.
We can compute $F(t)$ w.r.t. the walkers' PDF.

$F(t) = -\dfrac{dS}{dt} = -\dfrac{d}{dt}\int_{x_{min}}^{x_{max}}P(x,t) dx$

$= \int_{x_{max}}^{x_{min}}\dfrac{\partial P(x,t)}{\partial t} dx$

$= D\int_{x_{max}}^{x_{min}} \dfrac{\partial^2 P(x,t)}{\partial x^2}dx$

$\therefore \; \boxed{F(t) = D \left.\dfrac{\partial P}{\partial x}\right\|\_{x\_{max}} - D \left.\dfrac{\partial P}{\partial x}\right\|\_{x_{min}}}$

#### 2D First-Passage PDF
We can generalize the First-Passage PDF in more dimensions. In 2D, the diffusion equation is:

$\dfrac{\partial P}{\partial t} = D\left(\dfrac{\partial^2 P}{\partial x^2} + \dfrac{\partial^2 P}{\partial y^2}\right)$

The survival probablity integrates over a line $\Omega$:

$S(t) = \int_{\Omega}P(x,y,t)dx dy$

For the first-passage PDF:

$F(t) = -\dfrac{dS}{dt} = -\int_{\Omega}\dfrac{\partial P}{\partial t}dxdy$

$=-D\int_{\Omega}\left(\dfrac{\partial^2 P}{\partial x^2} + \dfrac{\partial^2 P}{\partial y^2}\right)dxdy$

Applying Green's theorem:

$F(t) = D \oint_{\partial \Omega} \left(\dfrac{\partial P}{\partial n} \right) dS$

* $\partial \Omega$ is the absorbing boundary (line) of the region.
* $\tfrac{\partial P}{\partial n}$ is the normal derivative at the boundary.
* $dS$ is an infinitesimal line element.

#### 3D First-Passage PDF

In 3D, the diffusion equation is written:

$\dfrac{\partial P}{\partial t} = D\left(\dfrac{\partial^2 P}{\partial x^2} + \dfrac{\partial^2 P}{\partial y^2 + \dfrac{\partial^2 P}{\partial z^2}\right)$

similarly to the 2D case, the first-passage PDF will end up as:

$F(t)=-D\int_{\Omega}\left(\dfrac{\partial^2 P}{\partial x^2} + \dfrac{\partial^2 P}{\partial y^2} + \dfrac{\partial^2 P}{\partial z^2}\right)dxdydz$

Applying the divergence theorem:

$F(t) = D \oint_{\partial \Omega} \left(\dfrac{\partial P}{\partial n} \right) dS$

* $\partial \Omega$ is the absorbing boundary (surface) of the region.
* $\tfrac{\partial P}{\partial n}$ is outward normal derivative at the boundary (flux).
* $dS$ is an infinitesimal surface element.

Therefore the general equation for the first-passage PDF is:

$\boxed{F(t) = D \oint_{\Omega} \left(\dfrac{\partial P}{\partial n} \right) dS}$

$\Omega$ refers to the cluster's absorbing (sticky) boundary. Since $\tfrac{\partial P}{\partial n} = \nabla P \cdot \hat{\textbf{n}}$, 
(see A.2), $F(t)$ is written as $F(t) = D \oint\_{\Omega} \nabla P \cdot \hat{\textbf{n}} dS = D \oint\_{\Omega} \textbf{J} \cdot \hat{\textbf{n}}dS$.
The latter is an integral of the product $\textbf{J}\cdot \textbf{n}$ and the product  $\textbf{J}\cdot \textbf{n}$ is a form of flux (see A.2).
This flux expresses the rate at which probability is absorbed by the boundary $\Omega$.
The integral expresses the total amount of probability absorbed by the cluster at time $t$.


### 3.2.1 First-Passage Time

## 3.3. Fractal Growth

We have examined the phenomenom of diffusion but not the aggregation. As a reminder, in DLA, aggregation 
says that if a particle sticks to the aggregate cluster, it cannot ever move again. Imagine particles as tiny bacteria 
moving randomly until they touch a colony and stick. If we observe DLA, it turns out that 
it naturally leads to fractal growth because of the way particles randomly diffuse and attach to a growing cluster.
DLA has the following characteristics found in fractals:

1. It is irreversible - particles stick to the surface upon first contact.
2. Self-similarity at different scales - zooming at different regions of the cluster, it looks similar to the whole.
3. Formation of dense and sparse regions - as they diffuse, particles tend to stick to the tips of the cluster, forming
fjords and valleys.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/fractal_sander.png" alt="" width="300">

[*ref*](https://www.thp.uni-koeln.de/krug/teaching-Dateien/SS2012/Sander2000.pdf)

**TODO**: power law etc.

# 4. Simulating DLA in C

# 5. Thoughts

# A.1 Discrete Derivatives

# A.2 Surface Integrals In A Nutshell

## A.2.1 Surface Parametrization

### A.2.1.1 Explicit vs Implicit Surfaces

Before going in flux integrals, it's important to know how 3D surfaces are expressed.
There are two ways of expressing surfaces - explicitly and implicitly. The explicit
(and simpler) way is by the equation 

$\displaystyle z = f(x,y)$

Surfaces represented this way are called **graphs**. This form cannot express vertical
or multivariate surfaces where one $(x,y)$ maps to more than one $z$, such as
the sphere $x^2+y^2+z^2=4$ or the cylinder $x^2+y+2=4, \ z=z$. It can represent e.g. a 
paraboloid $z=x^2 + y^2$ or the cone $z=\sqrt{x^2 + y^2}$. This form makes it easier
to compute the derivatives $f_x, \ f_y$ and the normal vector at some $(x,y,z)$.

The implicit form describes a surface as:

$\displaystyle F(x,y,z) = 0$

This form can additionally describe closed surfaces, such as the sphere $x^2+y^2+z^2 = 1$, the
ellipsoid $x^2 + 2y^2 + 4z^2 = 16$, or the cylinder $x^2 + y^2 = 1, \forall\ z: \ -2 \leq z \leq 5$. However, it
makes integrating over it more difficult, as the surface typically needs to be 
parametrized (e.g. in polar coordinates) first.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/various_surfaces.png" alt="" width="700">

### A.2.1.2 Parametrizing an Implicit Surface

[ref](https://www.math.purdue.edu/~neptamin/324Au17/Notes/16.6/16.6.pdf)

Sometimes it is easier to describe a surface by means of a position vector instead of 
their explicit/implicit equation, especially when we're dealing with surfaces of 
revolution (e.g. paraboloids, spheres, ellipses, etc.). A **position vector** $\textbf{r}$ is one
whose origin coincides with the coordinate system's origin and its end maps to every point
of the surface. To write the parametrization of a surface, we pick two new variable $(u,v)$ and
write $x, y, z$ with respect to them, defining the position vector (or $(x,y,z) \mapsto \left(x(u,v), y(u,v), z(u,v) \right)$ 
as:

<table style="width: 100%; border-collapse: collapse; border: none;">
  <tr>
    <td style="width: 50%; text-align: center; border: none;">$\displaystyle x= x(u,v),\; y= y(u,v),\; z= z(u,v)$</td>
    <td style="width: 50%; text-align: center; border: none;">$\displaystyle \textbf{r}(u,v) = \big(x(u,v), y(u,v), z(u,v) \big)$</td>
  </tr>
</table>

Parametrizations are not unique and we are free to pick $u, v$ as long as the new $x:=x(u,v),\ y:=y(u,v),\ z:=z(u,v)$ 
satisfy the surface's equation $F(x,y,z) = 0$. However in the case of the explicit surface $z=f(x,y)$, parametrization
is easier since after picking $x:=x(u,v), \ y:=y(u,v)$, we can just obtain $z=f\big(x(u,v), y(u,v)\big)$ and write the
position vector as:

$\textbf{r}(u,v) = \big(x(u,v), y(u,v), f(x,y) \big)$

Some common ways to parametrize a surface depending on its symmetry
are via polar, cylindrical, or spherical coordinates:

<table style="width: 100%; border-collapse: collapse; border: none;">
  <tr>
    <td style="width: 33%; text-align: center; border: none;">polar:</td>
    <td style="width: 33%; text-align: center; border: none;">cylindrical:</td>
    <td style="width: 33%; text-align: center; border: none;">spherical:</td>
  </tr>
  <tr>
    <td style="width: 33%; text-align: center; border: none;">$x(r,\theta) = r\cos(\theta)$</td>
    <td style="width: 33%; text-align: center; border: none;">$x(r,\theta) = r\cos(\theta)$</td>
    <td style="width: 33%; text-align: center; border: none;">$x(\phi, \theta) = r\sin(\phi)\cos(\theta)$</td>
  </tr>
  <tr>
    <td style="width: 33%; text-align: center; border: none;">$y(r,\theta) = r\sin(\theta)$</td>
    <td style="width: 33%; text-align: center; border: none;">$y(r,\theta) = r\sin(\theta)$</td>
    <td style="width: 33%; text-align: center; border: none;">$y(\phi, \theta) = r\sin(\phi)\sin(\theta)$</td>
  </tr>
  <tr>
    <td style="width: 33%; text-align: center; border: none;">$0 \leq \theta \leq 2\pi$</td>
    <td style="width: 33%; text-align: center; border: none;">$0 \leq \theta \leq 2\pi$, $z = r $</td>
    <td style="width: 33%; text-align: center; border: none;">$z = r\cos(\phi), \ r=const$</td>
  </tr>
  <tr>
    <td style="width: 33%; text-align: center; border: none;"></td>
    <td style="width: 33%; text-align: center; border: none;"></td>
    <td style="width: 33%; text-align: center; border: none;">$0 \leq \theta \leq 2\pi, \; 0 \leq \phi \leq \pi$</td>
  </tr>
</table>

In the case of mapping to spherical coordinates, $r$ can be obtained from the equation $F(x,y,z)$.
In the same coordinates, $\phi$ is the latitude angle (with the $z$ axis) and $\theta$ the longitude
(horizontal plane) angle).

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/cartesian_to_spherical.jpg" alt="" width="300">

#### Example

Find a parametrization of the following surfaces:

1. the upper cone $z = \sqrt{x^2 + y^2}$
2. the upper cone $y = \sqrt{x^2 + 2z^2}$
3. the lower cone $z = -2\sqrt{x^2 + 2y^2}$
4. the cylinder $x^2 + y^2 = 4, \; 0 \leq z \leq 1$
5. the paraboloid $z = x^2 + y^2$
6. the sphere $x^2 + y^2 + z^2 = a^2$
7. the ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$
8. the torus $(x^2 + y^2 + z^2 + R^2 - r^2) = 4R^2(x^2 + y^2)$

#### Solution 

<table style="width: 100%; border-collapse: collapse; border: none;">
  <tr>
    <td style="width: 10%; text-align: left; border: none;">1.</td>
    <td style="width: 90%; text-align: center; border: none;">$x(r,\theta) := r\cos(\theta),\ y(r,\theta) := r\sin(\theta)$ $\therefore \ z=\sqrt{x^2+y^2} = r$</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;"></td>
    <td style="width: 90%; text-align: center; border: none;">position vector: $\textbf{r}(r, \theta) = r\cos(\theta)\textbf{i} + r\sin(theta)\textbf{j} + r\textbf{k} $ </td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">2.</td>
    <td style="width: 90%; text-align: center; border: none;">$x(r,\theta) := r\cos(\theta),\ z(r,\theta) := r\sin(\theta)/\sqrt{2}$ $\therefore \ y=\sqrt{x^2+z^2} = r$</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;"></td>
    <td style="width: 90%; text-align: center; border: none;">position vector: $\textbf{r}(r, \theta) = r\cos(\theta)\textbf{i} + r\textbf{j} + r\frac{\sin(\theta)}{\sqrt{2}}\textbf{k} $</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">3.</td>
    <td style="width: 90%; text-align: center; border: none;">$x(r,\theta) := r\cos(\theta),\ y(r,\theta) := r\sin(\theta)/\sqrt{2}$ $\therefore \ z=\sqrt{x^2+y^2} = -r$</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">4.</td>
    <td style="width: 90%; text-align: center; border: none;">$x = 2\cos u, \ y = 2\sin u, \ z = v, \ 0 \leq z \leq 1$</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">5.</td>
    <td style="width: 90%; text-align: center; border: none;">$x = r\cos(\theta),\ y = r\sin(\theta)$ $\therefore \ z=\sqrt{x^2+y^2} = r^2$</td>
  </tr>
  <tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">6.</td>
    <td style="width: 90%; text-align: center; border: none;">$x = \sin(\phi)\cos(\theta)$, $y = \sin(\phi)\sin(\theta)$, $z = c\cos(\phi)$, $0\leq \theta \leq 2\pi$, $0 \leq \phi \leq \pi$</td>
  </tr>
    <td style="width: 10%; text-align: left; border: none;">7.</td>
    <td style="width: 90%; text-align: center; border: none;">$x = a\cos\theta \ sin\phi, \; y = b\sin\theta\ \sin\phi, \; z = c\cos\phi$</td>
  </tr>
  <tr>
    <td style="width: 10%; text-align: left; border: none;">8.</td>
    <td style="width: 90%; text-align: center; border: none;">$x = (R + r\cos\phi)\cos\phi$, $y=(R+r\cos\phi)\sin\phi$, $z = r\sin\phi$, $0 \leq \phi, \theta \leq 2\pi$</td>
  </tr>
</table>


## A.2.1 Flux Integrals 

Flux integrals often appear in DLA and are of the form $\displaystyle \iint_S \textbf{F}(x,y,z) \cdot \hat{\textbf{n}}dS$.
$\hat{\textbf{n}}$ is the oriented unit normal at each point on the surface and $dS$ the area element.
Remember that $\textbf{F}$ must be a vector field, otherwise they measure the total of a scalar over a surface (e.g. if 
$F$ is density scalar, they measure the surface's mass), or if $F$ is missing, they measure the total area of a surface.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/surface_integral_types.png" alt="" width="600">

We typically parametrize $\textbf{F}$ from $(x,y,z)$ to $(u,v)$ before integrating to reduce the number of variables.

### A.2.1.1 Normal To a Surface and Area Approximation

Before evaluating integrals of the form $\iint_S \text{something}dS$, two things must be calculated:

* the normal vector $\hat{\textbf{n}}$ (for flux integrals)
* an approximation to the surface area element $dS$

To do this, after parametrizing a surface as $\textbf{r}(u,v) = x(u,v)\textbf{i} + y(u,v)\textbf{j} + z(u,v)\textbf{k}$,
we compute the two tangent vectors at each point, which are given by the partial derivative along each direction $u$, $v$:

$\displaystyle \textbf{r}_u = \frac{\partial \textbf{r}}{\partial u} = x_u\textbf{i} + y_u\textbf{j} + z_u\textbf{k}$

$\displaystyle \textbf{r}_v = \frac{\partial \textbf{r}}{\partial v} = x_v\textbf{i} + y_v\textbf{j} + z_v\textbf{k}$

The normal vector is then given by their cross product and to integrate we normalize it:

<center>$\displaystyle \boxed{\hat{\textbf{n}} = \frac{\textbf{r}_u \times \textbf{r}_v}{\left\| \textbf{r}_u \times \textbf{r}_v \right\|}}$</center>

#### Example

[ref](https://www.math.purdue.edu/~neptamin/324Au17/Notes/16.6/16.6.pdf)

Find the tangent plane to the unit surface at $\left(\frac{1}{2}, \frac{1}{2}, \frac{\sqrt{2}}{2}\right)$.

#### Solution 

Paramtrize the sphere as:

<center>$\displaystyle \textbf{r}(\theta, \phi) = \sin(\phi)\cos(\theta)\textbf{i} + \sin(\phi)\sin(\theta)\textbf{j} + \cos(\phi)\textbf{k}$</center>

with $0\leq \theta \leq 2\pi$ and $0 \leq \phi \leq \pi$. Then the partial derivative vectors are:

<center>$\displaystyle \textbf{r}_{\theta} = -\sin(\phi)\sin(\theta)\textbf{i} + \sin(\phi)\cos(\theta)\textbf{j} + 0\textbf{k}$</center>

<center>$\displaystyle \textbf{r}_{\phi} = \cos(\phi)\cos(\theta)\textbf{i} + \cos(\phi)\sin(\theta)\textbf{j} - \sin(\phi)\textbf{k}$ </center>

The point of interest $\left(\frac{1}{2}, \frac{1}{2}, \frac{\sqrt{2}}{2}\right)$ where we want to find the tangent place corresponds to 
$(\theta, \phi) = (\frac{\pi}{4}, \frac{\pi}{4})$. Therefore at that point:

<center>$\displaystyle \textbf{r}_{\theta} \left(\frac{\pi}{4}, \frac{\pi}{4}\right)= -\frac{1}{2}\textbf{i} + \frac{1}{2}\textbf{j} + 0\textbf{k}$,</center>

<center>$\displaystyle \textbf{r}_{\phi} \left(\frac{\pi}{4}, \frac{\pi}{4}\right)= \frac{1}{2}\textbf{i} + \frac{1}{2}\textbf{j} - \frac{\sqrt{2}}{2}\textbf{k}$</center>

The normal $\textbf{n}$ is given by their cross product $\textbf{r}_u \times \textbf{r}_v$:

$\textbf{n} = \textbf{r}_u \times \textbf{r}_v =$ $$ \begin{vmatrix}\textbf{i} & \textbf{j} & \textbf{k} \\ -\frac{1}{2} & \frac{1}{2} & 0 \\ \frac{1}{2} & \frac{1}{2} & \frac{\sqrt{2}}{2}\end{vmatrix}=$$ $-\frac{\sqrt{2}}{4}\textbf{i} - \frac{\sqrt{2}}{4}\textbf{j} - \frac{1}{2}\textbf{k} $

If we draw a line from any point $\textbf{x}=(x,y,z)$ of the tangent plane to the origin $\textbf{p}=\left(\frac{1}{2}, \frac{1}{2}, \frac{\sqrt{2}}{2}\right)$ of $\textbf{n}$, then this vector is always normal to $\textbf{n}$, which describes the tangent plane equation:

<center>$(\textbf{x} - \textbf{p})\cdot \textbf{n} = 0 \Rightarrow \frac{\sqrt{2}}{4}\left(x - \frac{1}{2}\right) + \frac{\sqrt{2}}{4}\left(y - \frac{1}{2}\right) + \frac{1}{2}\left(z - \frac{\sqrt{2}}{2}\right) = 0$ </center>

Next, we need to approximate the surface area element $dS$ in the domain $(u,v)$ with a parallelogram tangential to each point at that domain.
Therefore the edges of the parallelogram will be made of the tangential vectors $\textbf{r}_u$ (obtained by 
keeping $v$ constant along a line on the surface and varying $u$) and $\textbf{r}_v$ ($u$ constant, varying $v$).

Recall from the properties of the cross product that the area of a parallelogram is with edges $\textbf{a}$, $\textbf{b}$
is given by $\left| \textbf{a} \times \textbf{b}\right|$. Therefore the area of the parallelogram to approximate $dS$ is 
$\left| \textbf{r}_u \times \textbf{r}_u\right|dA$, where $dA$ is an infiniteseminal planar patch in the $(u,v)$
domain. Vectors $\textbf{r}_u, \ \textbf{r}_u$ stretch it and map it to the parallelogram. 

Note that $dA$ is NOT ALWAYS $du \ dv$! This is true only for flat rectangular domains,
Note that when we change coordinate INSIDE THE INTEGRAL, e.g. from $(x,y,z)$ to $(u,v)$, we need to take into account
the **Jacobian** matrix $J(u,v)$ of the new coordinates by multiplying $dA$ with its absolute determinant $\left|\left|J(u,v)\right|\right|$.
The Jacobian encodes how a function stretches, rotates, skews, or flips space locally when we transform from one
coordinate space to another.

<center>$\boxed{dS \approx \left|\textbf{r}_u \times \textbf{r}_v \right| \cdot \underbrace{\left|J(u,v)\right|}_{\substack{\text{if change} \\ \text{coords}}} du dv}$</center>


<table style="border-collapse: collapse; border: none; width: auto;">
  <tr>
    <td style="border: none; vertical-align: center;">
      <figure>
        <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/concave_surface_tangents.png" width=420 alt="Figure 1" style="max-width: 100%;">
        <figcaption>Figure TODO: visualization of tangent vectors on a surface.</figcaption>
      </figure>
    </td>
    <td style="border: none; vertical-align: top; padding-left: 20px;">
      <figure>
        <img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/plane_patch_approx.png" width=350 alt="Figure 2" style="max-width: 100%;">
        <figcaption>Figure TODO: Approximating a surface element via a flat patch.</figcaption>
      </figure>
    </td>
  </tr>
</table>

In the special case of a graph, its surface is descrined by $z=f(x,y)$ therefore we can parametrize it as
$\textbf{r}(x,y) = x\textbf{i} + y\textbf{j} + f(x,y)\textbf{k}$. The tangetial vectors are therefore
$\textbf{r}_x = \textbf{i} + 0\textbf{j} + f_x(x,y)\textbf{k}$ and $\textbf{r}_y = 0\textbf{i} + \textbf{j} + f_y(x,y)\textbf{k}$.
Their cross product is:

$\textbf{r}_x \times \textbf{r}_y = $ $$ \begin{vmatrix}\textbf{i} & \textbf{j} & \textbf{k} \\ 1 & 0 & f_x(x,y) \\ 0 & 1 & f_y(x,y)\end{vmatrix}$$ $= -f_x(x,y)\textbf{i} - f_y(x,y)\textbf{j} + \textbf{k}$

Its magnitude is then $\left\|\textbf{r}_x \times \textbf{r}_y\right\| = \sqrt{1 + f_x^2 + f_y^2}$ and the area element is approximately:

<center>$dS \approx \sqrt{1 + f_x^2 + f_y^2} \underbrace{\left|J(u,v)\right|}_{\substack{\text{if change} \\ \text{coords}}} du dv$</center>

The last quantity to define is the Jacobian. This helps measure the area element in the target domain 
if we change coordinates inside the integral.
For a transform $(x,y,z) \rightarrow (u,v)$ of $x(u,v), y(u,v), z(u,v)$,
the Jacobian is a matrix of first order partial derivatives and we evaluate its determinant, which is:

$\|J(u,v)\| =$ $$\left \|\begin{matrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \\ \frac{\partial z}{\partial u} & \frac{\partial z}{\partial v}\end{matrix} \right \|$$

For example, for the cylindrical transform $(x,y,z) \rightarrow (r, \theta, z)$ with $x=r\cos\theta$,
$y = r\sin\theta$, $z = z$, the Jacobian matrix is:


$J(r, \theta, z) =$ 
$$
\begin{bmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial z} \\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial z} \\
\frac{\partial z}{\partial r} & \frac{\partial z}{\partial \theta} & \frac{\partial z}{\partial z}
\end{bmatrix}
=
\begin{bmatrix}
\cos\theta & -r\sin\theta & 0 \\
\sin\theta & r\cos\theta & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

Its determinant is:

$\left|J(r, \theta, r)\right| = $
$$
\begin{vmatrix} \cos\theta & -r\sin\theta & 0 \\ \sin\theta & r\cos\theta & 0 \\ 0 & 0 & 1 \end{vmatrix} = 
\cos \theta \cdot r \cos \theta - (-r\sin\theta \cdot \sin\theta) = r
$$

The area element in cylindrical coordinates is therefore $dA = \|\|J(u,v)\|\|drd\theta = r dr d\theta$

This can be verified geometrically. Note that the second term that depends on $dr^2$ is neglibible
compare to the one that depends on $dr$.

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/polar_coords_area.png" alt="" width="550">


### A.2.1.2 Total Area Integrals 

Finding the total area of a surface is the simplest application of surface integrals.
In this case we compute $\iint dS$ and just need to substitute $dS$.

#### Example
Calculate the surface area of that portion of the paraboloid $z=x^2+y^2$ defined 
over $D:\ 0 \leq x^2 + y^2 \leq 4$.

#### Solution

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/paraboloid_example.png" alt="" width="300">

We'll parametrize in polar coordinates with $x = r\cos\theta, \; y = r\sin\theta$, 
$r = \sqrt{x^2 + y^2} = \sqrt{z}$, so the position vector and area element are:

Because $z(r,\theta)$ is a graph, the area is given by the simplified formula.
The boundaries between planes $z=0$ and $z=4$ in the $(x,y)$ plane will be given by 
the boundaries from $r=0$ to $r=\sqrt{4} = 2$ and for angles $0 \leq \theta \leq 2\pi$.

$\displaystyle Area(S) = \int_0^2 \int_0^{2\pi} dS = \int_0^2 \int_0^{2\pi} \sqrt{1 + f_x^2 + f_y^2} dA = \int_0^2 \int_0^{2\pi} \sqrt{1 + 4x^2 + 4y^2}\, r\,dr\,d\theta$

To make the computation easier, we trasnform the integrant in polar coordinates, therefore 
use the Jacobian. The area element in the polar domain is 
$dA=|J(u,v)|dr d\theta = rdrd\theta$. We express $\sqrt{.}$ w.r.t. to $(r,\theta)$, i.e.
$\sqrt{4x^2 + 4y^2 + 1} = \sqrt{4r^2\cos^2\theta + 4r^2\sin^2\theta + 1} = \sqrt{4r^2 + 1}$.
The integral can now be evaluated using $r$ and $\theta$:

$\displaystyle Area(S) = \int_0^2 \int_0^{2\pi} \sqrt{4r^2 + 1} \,r\,dr\,d\theta = 2\pi \int_0^2 \sqrt{4r^2 + 1} \,r\,dr\, d\theta = \frac{\pi}{6}\left(17^{3/2} -1 \right)$

#### Example


[ref](https://users.math.msu.edu/users/gnagy/teaching/10-fall/mth234/w13-234-h.pdf)
Find the area of the region cut from the plane $x + 2y + 2z = 5$ by the parabolas
with $x = y^2$ and $x = 2-y^2$.

#### Solution

$z$ is a graph and we do not need to parametrize it, therefore no Jacobian is needed.
Because it's a graph, the area is given by the simplified J-free formula:

<center>$\displaystyle Area(S) = \iint_S \sqrt{1 + f_x^2 + f_y^2} dA = \iint_S \sqrt{1 + \frac{1}{2^2} + 1} dx dy$</center>

What's remaining is to find the bounds of region $S$ on the $xy$ plane. Sketching the walls of the region
helps. Obviously $2-y^2 \leq x \leq y^2$. The two parabolas $x=y^2$ and $x=2-y^2$ cross at $(1, -1)$ and $(1,1)$
therefore $-1 \leq y \leq 1$. The integral then becomes:

<center>$\displaystyle Area(S) = \frac{3}{2} \int_{-1}^2\int_{y^2}^{2-y^2} dx dy = 4$</center>

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/plane_cut_by__cylinders.png" alt="" width="450">

#### Example

Find the total area of the cone with equation $z^2 = x^2 + y^2$.

#### Solution

We parametrize it with polar coordinates $x(r,\theta) = r\cos\theta$, $y(r, \theta) = r\sin\theta$,
$z^2 = r^2 \; \therefore \; z = r$ so the position vector for the cone is

<center>\textbf{r}(r, \theta) = r\cos\theta\textbf{i} + r\sin\theta\textbf{j} + r\textbf{k}</center>

The tangential vectors and area element are:

<center>$\textbf{r}_r = \cos\theta\textbf{i} + \sin\theta\textbf{j} + \textbf{k}$</center>

<center>$\textbf{r}_{\theta} = -\sin\theta\textbf{i} + \cos\theta\textbf{j} + 0{k}$</center>

<center>$||\textbf{r}_r \times \textbf{r}_{\theta} || = \ldots = r\sqrt{2}$</center>

<center>$dA = ||\textbf{r}_r \times \textbf{r}_{\theta} || = r\sqrt{2} dr d\theta$</center>

The total area is

<center>$\displaystyle S = \int_0^{2\pi} \int_0^R dA = \int_0^{2\pi} \int_0^R r\sqrt{2} dr d\theta = \pi \sqrt{2} R^2$</center>

$R$ is the height of the cube, which is also the projection of its rim on the $xy$ plane (radius).

[todo cone area](https://www.math.uci.edu/~remote_teaching/Lecture_Notes_of_Hamed/Math%202E%20Lecture%20Notes/Lecture%20Note%2016-6.pdf)

[todo](https://users.math.msu.edu/users/gnagy/teaching/10-fall/mth234/w13-234-h.pdf)


[optional todo - sphere area](https://www.math.purdue.edu/~neptamin/324Au17/Notes/16.6/16.6.pdf)

### A.2.1.3 Mass Integrals 

[ref](https://www.whitman.edu/mathematics/calculus_online/section15.03.html)

[ref](https://users.math.msu.edu/users/gnagy/teaching/10-fall/mth234/w13-234-h.pdf)


[ref](https://users.math.msu.edu/users/gnagy/teaching/10-fall/mth234/w13-234-h.pdf)

Integrals of the form $\iint f(x,y,z) dS$ integrate on a weighted surface $S$, where $f(x,y,z)$ represents the weight at each point.

One common application where these arise is center of mass computations. The center of mass corresponds to a point in space 
where all the mass of a surface would be concentrtated if it was to be represented by a single point. If we placed a spindle 
under the center of a surface (e.g. a lamina), then it would perfectly balance. 

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/lamina_center_of_mass.jpeg" alt="" width="400">
<small>A lamina is perfectly balances on a spindle if the laminas center of mass sits on the spindle.</small>

Before introducing the center of mass of a surface, it helps to understand the center of mass of a 
group of particles. The center of mass is the point in space that makes the sum of the torques of all 
masses zero. The torque of a mass about some point is $mass \, \times \, distance$. Imagine three masses 
of $10, 5, 4$ kg placed at $x=3,6,8$ m respectively on a rigid beam of neglibible weight. Then the center of mass 
is at $\overline{x} \approx 4.84$ as:

<center>$\displaystyle 10(3-\overline{x}) + 5(6-\overline{x}) + 4(8-\overline{x}) = 0 \Rightarrow$</center>

<center>$\overline{x} = 92/19 \approx 4.84 \Rightarrow$</center>

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/beam_three_masses.png" alt="" width="400">



The surface represents a thin shell in space,
$f(x,y,z)$ represents a density function for each point at the surface and it's denoted by $\rho: S \rightarrow \mathbb{R}$.
The mass $M$ of a thin shell described by surface $S$ in space with mass per unit area function $\rho : S \rightarrow \mathbb{R}$
is given by

$\displaystyle M = \iint_S \rho dS$

The center of mass $\textbf{x}_c = (x_c, y_c, z_c)$ of the thin shell above is 

$\displaystyle x_c = \frac{1}{M}\iint x \rho \, dS, \quad y_c = \frac{1}{M}\iint y \rho \, dS, \quad z_c = \frac{1}{M}\iint z \rho \, dS$


#### Example

Find the center of mass of a two-dimensional plate that occupies the quarter circle $x^2+y^2\leq 1$ in the 
first quadrant and has density $k(x^2 + y^2)$.

#### Solution

$\displaystyle M = \int_0^1 \int_0^{\sqrt{1-x^2}} k(x^2 + y^2) dy dx$

The latter is easier to compute by switching to polar coordinates ($x^2 + y^2 = r^2$).

$\displaystyle M = \int_0^{\pi/2} \int_0^1 k r^2 \ rdrd\theta = k \int_0^{\pi/2} \left. \frac{r^4}{4}\right|^1_0 = k \int_0^{\pi/2}\frac{1}{4}d\theta = k \frac{\pi}{8}$



### A.2.1.4 Flux Integrals 






### A.2.1.2 Area and Flux Integrals

**Flux** is a quantity that measures how much a vector field goes *across* a boundary (surface or a line).
Flux is signed, so the field can cross a boundary inwards or outwards. The more perpendicular a vector field 
is against a boundary, the higher the flux. If they align, the flux is zero.

To measure the flux, we are given a vector field $\textbf{F}$ and a curve $C$ or a surface $S$. 
Given a continuous flow field $\textbf{F}$ and a smooth (differentiable everywhere), simple 
(does not intersect itself) closed curve $C$ (or surface $S$), the flux is defined as:

<table style="width: 100%; border-collapse: collapse; border: none;">
  <tr>
    <td style="width: 50%; text-align: center; border: none;">$\displaystyle Flux = \int_{C}\textbf{F} \cdot \hat{\textbf{n}} \ dS$</td>
    <td style="width: 50%; text-align: center; border: none;">$\displaystyle Flux = \iint_{S}\textbf{F} \cdot \hat{\textbf{n}} \ dS$</td>
  </tr>
</table>




If $F(x,y) = M(x,y)\textbf{i} + N(x,y)\textbf{j}$ is a vector field on a plane, then the flux is written as:
 
$\displaystyle Flux = \int_{C} \left(M(x,y)n_x + N(x,y)n_y\right) dS$

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/flux_line_surfaces.png" alt="" width="400">

As you can see in the figure above, a normal vector at any point on a smooth surface can be obtained by the cross product 
of the two partial derivatives. To normalize it, we furthermore divide by its magnitude:

$\hat{\textbf{n}} = \frac{F_x\textbf{i} \times F_y\textbf{j}}{\left| F_x\textbf{i} \times F_y\textbf{j}\right\}$

#### Example

Let $F(x,y) = x\textbf{i} + y\textbf{j}$ and suppose that $C$ is a circle of radius $R$. 
Find the flux of $F$ across the circle.

#### Solution

The field $F$ is composed of straigh lines through the origin. Therefore at every intersection it's perpendicular
to the circle. In this case, $\textbf{F} \cdot \hat{\textbf{n}} = \left|\textbf{F}\right|\left|\hat{\textbf{n}}\right| = \left|\textbf{F}\right| = R$.
Therefore the flux is 
$\displaystyle \oint_C \textbf{F} \cdot \hat{\textbf{n}} \ dS = \oint_C R \ dS = 2\pi R^2$

In the case of a circle, finding the normal vector was trivial. However, $\hat{\textbf{n}}$ can be derived in general.
$\hat{\textbf{n}}$ is the normal vector that always points *outward* to the curve (or surface).

<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/normal_vector.png" alt="" width="450">



## A.2.2 Flux for radial symmetry 

The normal derivative of a vector function $P(\textbf{x})$ is the rate of change of $P$
in the direction of the perpendicular to its surface. If $\textbf{n}$ is the unit normal 
vector at some point on the surface S, the normal derivative is defined as:

$\dfrac{\partial P}{\partial n} = \nabla P(\textbf{x}) \cdot \hat{\textbf{n}}$

$\nabla P$ is the gradient of $P$. The shapes (e.g. the cluster and boundary) often have or 
ca be approximated by radial symmetry. Therefore let's see how the integral of a normal derivative 
along a circle is computed. This represents the flux through the circle.

In the case of radial symmetry (circle or sphere), the normal derivative is just the just the radial 
derivative:

$\boxed{\dfrac{\partial P}{\partial n} = \dfrac{\partial P}{\partial r}}$

To see why this is true, consider the gradient $\nabla P$ in polar coordinates $(r,\theta)$. Then it can 
be proven that

$\nabla P = \left(\dfrac{\partial P}{\partial r}, \dfrac{1}{r}\dfrac{\partial P}{\partial \theta} \right)$

However in polar coordinates, the normal vector $\hat{\textbf{n}}$ points outwards so it has no $\hat{\theta}$ component, i.e.

$\hat{\textbf{n}} = (1,0)$

The directional derivative is then:

$\dfrac{\partial P}{\partial n} = \nabla P \cdot \hat{\textbf{n}} = \left(\dfrac{\partial P}{\partial r}, \dfrac{1}{r}\dfrac{\partial P}{\partial \theta} \right) \cdot (1,0) = \dfrac{\partial P}{\partial r}$
 
<img src="https://raw.githubusercontent.com/leonmavr/leonmavr.github.io/refs/heads/master/_posts/2024-12-22-DLA/flux_values.png" alt="" width="600">

[ref](https://facultyweb.kennesaw.edu/mlavrov/courses/calculus-iv/lecture9.pdf)

The above formula comes in handy when evaluating the flux through radially symmetric surfaces.
If we integrate the normal derivative over a closed line (in 2D), we obtain the flux through it:

$Flux = \oint_{C}\dfrac{\partial P}{\partial n}dS$


#### Example 1 (flux though a circle)

Consider the circle $x^2+y^2=R^2$ and the decaying function $$

#### Example 2 (flux though an ellipse)


[polar gradient ref 1](https://www.youtube.com/watch?v=csF3EkH1hN4)

[polar gradient ref 2](https://profoundphysics.com/gradient-in-different-coordinates/)



